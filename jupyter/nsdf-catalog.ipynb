{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m pip install clickhouse-driver\n",
    "\n",
    "import os,sys\n",
    "from clickhouse_driver import connect,Client\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# you need a ~/.nsdf/vault/vault.yaml file\n",
    "from nsdf.kernel import NormalizeEnv, SetEnv\n",
    "env=NormalizeEnv({\n",
    "\t\"include-vault\": [\n",
    "\t\t\"s3-cloudbank\",\n",
    "\t\t\"clickhouse\"\n",
    "\t]\n",
    "})\n",
    "SetEnv(env)\n",
    "\n",
    "AWS_ACCESS_KEY_ID     = env[\"AWS_ACCESS_KEY_ID\"]\n",
    "AWS_SECRET_ACCESS_KEY = env[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "AWS_ENDPOINT_URL      = env[\"AWS_ENDPOINT_URL\"]\n",
    "\n",
    "CLICKHOUSE_HOST       = env[\"CLICKHOUSE_HOST\"]\n",
    "CLICKHOUSE_PORT       = env[\"CLICKHOUSE_PORT\"]\n",
    "CLICKHOUSE_USER       = env[\"CLICKHOUSE_USER\"]\n",
    "CLICKHOUSE_PASSWORD   = env[\"CLICKHOUSE_PASSWORD\"]\n",
    "\n",
    "BUCKET                = \"nsdf-catalog\"\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////\n",
    "def RunShellCommand(cmd):\n",
    "\timport shlex,subprocess\n",
    "\tprint(cmd)\n",
    "\tresult=subprocess.run(shlex.split(cmd), shell=False, check=False,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "\tprint(f\"Run {cmd} returncode={result.returncode}\")\n",
    "\tprint(result.stdout.decode('utf-8'))\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////\n",
    "def RunClientCommand(query, prefix_cmd=\"\", allow_errors=0):\n",
    "\tassert '\"'  not in query\n",
    "\tassert \"$\"  not in query\n",
    "\tcmd=f\"clickhouse-client  --host {CLICKHOUSE_HOST} --port {CLICKHOUSE_PORT} --secure --user {CLICKHOUSE_USER} --password {CLICKHOUSE_PASSWORD} --query=\\\"{query}\\\"\"\n",
    "\t\n",
    "\tif prefix_cmd:\n",
    "\t\tcmd=prefix_cmd + \" \" + cmd\n",
    "\n",
    "\tif allow_errors:\n",
    "\t\tcmd=cmd + f\" --input_format_allow_errors_num={allow_errors}\"\n",
    "\tRunShellCommand(cmd)\n",
    "\t\n",
    "# ////////////////////////////////////////////////////////////////////////\n",
    "def InsertRecordsFromS3(prefix, allow_errors=0):\n",
    "\t\"\"\"\n",
    "\thttps://dzone.com/articles/clickhouse-s3-compatible-object-storage\n",
    "\t\"\"\"\n",
    "\treturn RunClientCommand(f\"INSERT INTO nsdf.catalog SELECT * FROM s3('{prefix}','{AWS_ACCESS_KEY_ID}', '{AWS_SECRET_ACCESS_KEY}', 'CSV');\")\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////\n",
    "def InsertRecordsFromCSV(filename):\n",
    "\tRunClientCommand(\"INSERT INTO nsdf.catalog FORMAT CSV\", prefix_cmd=f\"cat {filename} |\")\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////\n",
    "def Connect():\n",
    "\treturn Client(host=CLICKHOUSE_HOST, port=str(CLICKHOUSE_PORT), user=CLICKHOUSE_USER, password=CLICKHOUSE_PASSWORD, secure=True)\n",
    "\n",
    "client = Connect()\n",
    "client.execute(\"\"\"CREATE DATABASE IF NOT EXISTS nsdf\"\"\")\n",
    "# client.execute(\"\"\"SHOW DATABASES\"\"\")\n",
    "# client.execute(\"\"\"DROP TABLE IF EXISTS nsdf.catalog\"\"\")\n",
    "client.execute(\"\"\"CREATE TABLE IF NOT EXISTS nsdf.catalog(\n",
    "\t\tcatalog         varchar(64),\n",
    "\t\tbucket          varchar(64),\n",
    "\t\tname            varchar(1024),\n",
    "\t\tsize            BIGINT,\n",
    "\t\tlast_modified   varchar(32) NULL,\n",
    "\t\tetag            varchar(256) NULL\n",
    "\t) \n",
    "\tENGINE = MergeTree() \n",
    "\tORDER BY(catalog,bucket)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check AWS credentials work\n",
    "if False:\n",
    "\tRunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/mc/\")\n",
    "\tRunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/aws-open-data/\")\n",
    "\tRunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/digitalrocksportal/projects/\")\n",
    "\tRunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/20220624/csv/mdf/\")\n",
    "\tRunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/arecibo/\")\n",
    "\tRunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/ranch/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of SQL INSERT (with statistics)\n",
    "\n",
    "# 25 seconds, 129,117 files, 5,533,076,354,245 (~5TB)\n",
    "# InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/mc/*.csv\")\n",
    "\n",
    "# 4m 51 seconds, 49,995,452 files, 10,531,224,818,071,513 (~10PB) todo... records I think are more than that\n",
    "# InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/aws-open-data/*.csv\")\n",
    "\n",
    "# 42 seconds, 8,638 files, 3,328,980,888,119 (~3TB)\n",
    "# InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/digitalrocksportal/projects/*.csv\")\n",
    "\n",
    "# 2m 39 seconds, 1,075,706 files, 5,243,595,789,858 (~5TB)\n",
    "# InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/20220624/csv/mdf/*.csv\")\n",
    "\n",
    "# <2m, 2,045,049 files, 491,912,368,698,644 total size (~500TB)\n",
    "# InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/arecibo/*.csv\")\n",
    "\n",
    "# <2m,  3,745,760 files, 36,750,124,831,337,770 (~34PB)\n",
    "# InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/ranch/*.csv\", allow_errors=9999999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total count per catalog\n",
    "client.execute(\"\"\"\n",
    "select catalog,COUNT(size),SUM(size)/(1024*1024*1024)\n",
    "from nsdf.catalog\n",
    "group by catalog;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from pprint import pprint\n",
    "\n",
    "buckets=[it for it in client.execute(f\"SELECT DISTINCT catalog,bucket FROM nsdf.catalog\")]\n",
    "print(buckets[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of plot of filesize inside a dataset\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def PlotSizes(filename,sizes):\n",
    "\tsizes=sorted(sizes)\n",
    "\tipd = 1/plt.rcParams['figure.dpi'] \n",
    "\tplt.figure(figsize=(1024*ipd,768*ipd))\n",
    "\tplt.title(f\"{filename} #({len(sizes)}) m({sizes[0]}) M({sizes[-1]})\")\n",
    "\tplt.plot(range(len(sizes)), sorted(sizes))\n",
    "\tos.makedirs(os.path.dirname(filename),exist_ok=True)\n",
    "\tplt.savefig(filename)\n",
    "\tplt.show()\n",
    "\n",
    "for catalog, bucket in buckets:\n",
    "\tsizes=[it[0] for it in client.execute(f\"SELECT size FROM nsdf.catalog WHERE catalog='{catalog}' and bucket='{bucket}'\")]\n",
    "\tif not sizes: continue\n",
    "\tPlotSizes(filename=f\"/tmp/plots/{catalog}/{bucket}.png\",sizes=sizes)\n",
    "\n",
    "\t# remove the `break` if you want all the plots\n",
    "\tbreak "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total records 56M files 44PB\n",
    "client = Connect()\n",
    "TOT_FILES,TOT_BYTES=client.execute(f\"SELECT count(size),SUM(size)/(1024*1024*1024) FROM nsdf.catalog;\")[0]\n",
    "print(TOT_FILES,TOT_BYTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # total size per catalog/bucket\n",
    "client.execute(\"\"\"SELECT catalog,bucket, SUM(size) as TotSize\n",
    "FROM nsdf.catalog\n",
    "GROUP BY catalog,bucket\n",
    "ORDER BY TotSize DESC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of objeccts per catalog/bucket\n",
    "client.execute(\"\"\"\n",
    "SELECT catalog,bucket, COUNT(size) As NumObjects\n",
    "FROM nsdf.catalog\n",
    "group by catalog,bucket\n",
    "ORDER BY NumObjects DESC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of looking to a specific catalog,bucket\n",
    "client.execute(\"\"\"\n",
    "SELECT SUM(size) from nsdf.catalog \n",
    "WHERE catalog='aws-open-data' and bucket='noaa-cors-pds';\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIKE querry for looking into filenames\n",
    "client.execute(\"\"\"\n",
    "SELECT count(*) from nsdf.catalog\n",
    "where name like '%a%'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size per catalog\n",
    "client.execute(\"\"\"\n",
    "SELECT catalog,SUM(size)\n",
    "from nsdf.catalog\n",
    "group by catalog;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUM per bucket\n",
    "client.execute(\"\"\"\n",
    "select catalog,bucket,SUM(size)\n",
    "from nsdf.catalog\n",
    "group by catalog,bucket\n",
    "order by COUNT(size) DESC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT per bucket \n",
    "client.execute(\"\"\"\n",
    "select catalog,bucket,COUNT(size)\n",
    "from nsdf.catalog\n",
    "group by catalog,bucket\n",
    "order by COUNT(size) DESC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file size distribution \n",
    "client.execute(\"\"\"\n",
    "select size\n",
    "from nsdf.catalog\n",
    "where catalog='mc' and bucket='102'\n",
    "order by size ASC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file type\n",
    "client.execute(\"\"\"\n",
    "select substring_index(name,'.',-1), count(substring_index(name,'.',-1)) as FileExtension \n",
    "from nsdf.catalog\n",
    "where catalog='mc' and bucket='102'\n",
    "group by substring_index(name,'.',-1);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete records\n",
    "# ALTER TABLE nsdf.catalog DELETE WHERE 1=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!! rw.chcql1nlq37luu78jtmk.at.double.cloud 9440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('h5',),\n",
       " ('h5',),\n",
       " ('h5',),\n",
       " ('h5',),\n",
       " ('h5',),\n",
       " ('h5',),\n",
       " ('h5',),\n",
       " ('h5',),\n",
       " ('h5',),\n",
       " ('h5',)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.execute(\"\"\"\n",
    "SELECT splitByChar('.','giorgio.scorzelli.h5')[-1]\n",
    "FROM nsdf.catalog             \n",
    "ORDER BY size DESC\n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
