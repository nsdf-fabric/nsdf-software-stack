{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install clickhouse-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from clickhouse_driver import connect,Client\n",
    "\n",
    "if True:\n",
    "\t# you need a ~/.nsdf/vault/vault.yaml file\n",
    "\tsys.path.append(\"../\")\n",
    "\tfrom nsdf.kernel import NormalizeEnv, SetEnv\n",
    "\tSetEnv(NormalizeEnv({\n",
    "\t\t\"include-vault\": [\n",
    "\t\t\t\"s3-cloudbank\",\n",
    "\t\t\t\"clickhouse-altinity\"\n",
    "\t\t]\n",
    "\t}))\n",
    "else:\n",
    "\t# change as needed\n",
    "\tos.environ[\"AWS_ACCESS_KEY_ID\"]=\"xxxx\"\n",
    "\tos.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"yyyy\"\n",
    "\tos.environ[\"AWS_ENDPOINT_URL\"]=\"https://s3.us-west-1.amazonaws.com\"\n",
    "\n",
    "\t# credentials for clickhouse\n",
    "\tos.environ[\"CLICKHOUSE_HOST\"]=\"nsdf.cogesic.altinity.cloud\"\n",
    "\tos.environ[\"CLICKHOUSE_PORT\"]=\"9440\"\n",
    "\tos.environ[\"CLICKHOUSE_USER\"]=\"admin\"\n",
    "\tos.environ[\"CLICKHOUSE_PASSWORD\"]=\"zzzzz\"\n",
    "\n",
    "\n",
    "# credentials for object storage CSV files \n",
    "AWS_ACCESS_KEY_ID     = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "AWS_SECRET_ACCESS_KEY = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "AWS_ENDPOINT_URL      = os.environ[\"AWS_ENDPOINT_URL\"]\n",
    "\n",
    "# credentials for clickhouse\n",
    "CLICKHOUSE_HOST       = os.environ[\"CLICKHOUSE_HOST\"]\n",
    "CLICKHOUSE_PORT       = os.environ[\"CLICKHOUSE_PORT\"]\n",
    "CLICKHOUSE_USER       = os.environ[\"CLICKHOUSE_USER\"]\n",
    "CLICKHOUSE_PASSWORD   = os.environ[\"CLICKHOUSE_PASSWORD\"]\n",
    "\n",
    "BUCKET = \"nsdf-catalog\"\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////\n",
    "def RunShellCommand(cmd):\n",
    "\timport shlex,subprocess\n",
    "\tprint(cmd)\n",
    "\tresult=subprocess.run(shlex.split(cmd), shell=False, check=False,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "\tprint(f\"Run {cmd} returncode={result.returncode}\")\n",
    "\tprint(result.stdout)\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////\n",
    "def RunClientCommand(query, prefix_cmd=\"\", allow_errors=0):\n",
    "\tassert '\"'  not in query\n",
    "\tassert \"$\"  not in query\n",
    "\tcmd=f\"clickhouse-client  --host {CLICKHOUSE_HOST} --port {CLICKHOUSE_PORT} --secure --user {CLICKHOUSE_USER} --password {CLICKHOUSE_PASSWORD} --query=\\\"{query}\\\"\"\n",
    "\t\n",
    "\tif prefix_cmd:\n",
    "\t\tcmd=prefix_cmd + \" \" + cmd\n",
    "\n",
    "\tif allow_errors:\n",
    "\t\tcmd=cmd + f\" --input_format_allow_errors_num={allow_errors}\"\n",
    "\tRunShellCommand(cmd)\n",
    "\t\n",
    "# ////////////////////////////////////////////////////////////////////////\n",
    "def InsertRecordsFromS3(prefix, allow_errors=0):\n",
    "\t\"\"\"\n",
    "\thttps://dzone.com/articles/clickhouse-s3-compatible-object-storage\n",
    "\t\"\"\"\n",
    "\treturn RunClientCommand(f\"INSERT INTO nsdf.catalog SELECT * FROM s3('{prefix}','{AWS_ACCESS_KEY_ID}', '{AWS_SECRET_ACCESS_KEY}', 'CSV');\")\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////\n",
    "def InsertRecordsFromCSV(filename):\n",
    "\tRunClientCommand(\"INSERT INTO nsdf.catalog FORMAT CSV\", prefix_cmd=f\"cat {filename} |\")\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////\n",
    "def Connect():\n",
    "\treturn Client(host=CLICKHOUSE_HOST, port=str(CLICKHOUSE_PORT), user=CLICKHOUSE_USER, password=CLICKHOUSE_PASSWORD, secure=True)\n",
    "\n",
    "\n",
    "# connect and create the db schema\n",
    "if True:\n",
    "\tclient = Connect()\n",
    "\tclient.execute(\"\"\"CREATE DATABASE IF NOT EXISTS nsdf\"\"\")\n",
    "\t# client.execute(\"\"\"DROP TABLE IF EXISTS nsdf.catalog\"\"\")\n",
    "\tclient.execute(\"\"\"CREATE TABLE IF NOT EXISTS nsdf.catalog(\n",
    "\t\t\tcatalog\t       varchar(64),\n",
    "\t\t\tbucket\t       varchar(64),\n",
    "\t\t\tname            varchar(1024),\n",
    "\t\t\tsize            BIGINT,\n",
    "\t\t\tlast_modified   varchar(32) NULL,\n",
    "\t\t\tetag            varchar(256) NULL\n",
    "\t\t) \n",
    "\t\tENGINE = MergeTree() \n",
    "\t\tORDER BY(catalog,bucket)\n",
    "\t\"\"\")\n",
    "\t\n",
    "\tret=client.execute(\"\"\"SHOW DATABASES\"\"\")\n",
    "\tprint(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check AWS credentials work (taken from env)\n",
    "RunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/mc/\")\n",
    "#RunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/aws-open-data/\")\n",
    "#RunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/digitalrocksportal/projects/\")\n",
    "#RunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/20220624/csv/mdf/\")\n",
    "#RunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/arecibo/\")\n",
    "#RunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/ranch/\")\n",
    "#RunShellCommand(f\"aws s3 --endpoint-url {AWS_ENDPOINT_URL} ls s3://{BUCKET}/dataverse/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert from material commons\n",
    "# 25 seconds, 129,117 files, 5,533,076,354,245 (~5TB)\n",
    "InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/mc/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4m 51 seconds, 49,995,452 files, 10,531,224,818,071,513 (~10PB) todo... records I think are more than that\n",
    "InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/aws-open-data/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42 seconds, 8,638 files, 3,328,980,888,119 (~3TB)\n",
    "InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/digitalrocksportal/projects/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2m 39 seconds, 1,075,706 files, 5,243,595,789,858 (~5TB)\n",
    "InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/20220624/csv/mdf/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <2m, 2,045,049 files, 491,912,368,698,644 total size (~500TB)\n",
    "InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/arecibo/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <2m,  3,745,760 files, 36,750,124,831,337,770 (~34PB)\n",
    "InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/ranch/*.csv\", allow_errors=9999999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5 seconds ~1M records ~60TB\n",
    "InsertRecordsFromS3(f\"{AWS_ENDPOINT_URL}/{BUCKET}/dataverse/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddSuffix(number,type=\"\", base=1024):\n",
    "\tif number < (base**2): return str(number//(base**0))+type\n",
    "\tif number < (base**2): return str(number//(base**1))+\"K\"+type\n",
    "\tif number < (base**3): return str(number//(base**2))+\"M\"+type\n",
    "\tif number < (base**4): return str(number//(base**3))+\"G\"+type\n",
    "\tif number < (base**5): return str(number//(base**4))+\"T\"+type\n",
    "\treturn str(number//(base**5))+\"P\"+type\n",
    "\n",
    "def PrintInfo(name,count,size):\n",
    "\tprint(\"repository =\",name.ljust(19)\n",
    "\t\t,\" - number of files =\",AddSuffix(count,base=1000).ljust(7)\n",
    "\t\t,\" - total file size =\",AddSuffix(size,base=1024,type=\"B\").ljust(7))\n",
    "\n",
    "if True:\n",
    "\n",
    "\t# total count per catalog\n",
    "\tmy_table = client.execute(\"\"\"\n",
    "\tselect catalog,COUNT(size),SUM(size)\n",
    "\tfrom nsdf.catalog\n",
    "\tgroup by catalog;\n",
    "\t\"\"\")\n",
    "\n",
    "\ttot_count,tot_size= 0,0\n",
    "\tfor name,count,size in my_table:\n",
    "\t\ttot_count += count\n",
    "\t\ttot_size += size\n",
    "\tmy_table = [[\"all repositories\",tot_count,tot_size]]+my_table\n",
    "\n",
    "\tmy_table.sort(key=lambda x:-x[2])\n",
    "\trepository_table = list(my_table)\n",
    "\tfor name,count,size in my_table:\n",
    "\t\tPrintInfo(name,count,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from pprint import pprint\n",
    "\n",
    "# print all buckets\n",
    "buckets=[it for it in client.execute(f\"SELECT DISTINCT catalog,bucket FROM nsdf.catalog\")]\n",
    "print(buckets[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetFileNames (catalog = None, group_buckets = False, group_suffix = False):\n",
    "\tif group_buckets: \n",
    "\t\tgrouping = \"catalog, bucket\"\n",
    "\t\tselecting =\"catalog, bucket\"\n",
    "\t\theader=[\"repository\", \"bucket\", \"file count\", \"files sizes\"]\n",
    "\telif group_suffix:\n",
    "\t\tgrouping  = \"catalog, suffix\"\n",
    "\t\tselecting = \"catalog, suffix\"\n",
    "\t\theader=[\"repository\", \"suffix\",\"file count\", \"files sizes\"]\n",
    "\telse:\n",
    "\t\tgrouping=\"catalog, bucket, suffix\"\n",
    "\t\tselecting = \"catalog, bucket, suffix\"\n",
    "\t\theader=[\"repository\", \"bucket\", \"suffix\",\"file count\", \"files sizes\"]\n",
    "\n",
    "\tcatalog_restriction = f\"catalog='{catalog}'\" if catalog is not None else  \"1==1\"\n",
    " \n",
    "\tret = client.execute(f\"\"\"\n",
    "\t\tSELECT {selecting},count(size) as NumSize,SUM(size) as TotSize\n",
    "\t\tFROM (\n",
    "\t\t\tSELECT \n",
    "\t\t\t\tarrayStringConcat(['.',splitByChar('.',name)[-1]]) as suffix,\n",
    "\t\t\t\tsize as size , \n",
    "\t\t\t\tcatalog, \n",
    "\t\t\t\tbucket\n",
    "\t\t\tFROM \n",
    "\t\t\t\tnsdf.catalog\n",
    "\t\t\tWHERE {catalog_restriction} AND (\n",
    "\t\t\t\tname like '_%._'   OR \n",
    "\t\t\t\tname like '_%.__'  OR \n",
    "\t\t\t\tname like '_%.___' OR \n",
    "\t\t\t\tname like '_%.____'   )\n",
    "\t\t\t) derived_table \n",
    "\t\t\tGROUP BY {grouping} \n",
    "\t\t\tORDER BY NumSize DESC;\n",
    "\t\"\"\")\n",
    "\n",
    "\treturn [header, ret]\n",
    "\n",
    "if True:\n",
    "\theader,rows = GetFileNames()\n",
    "\tprint (header)\n",
    "\tfor I,row in enumerate(rows):\n",
    "\t\tprint(row)\n",
    "\t\tif I>=10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of plot of filesize inside a dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def PlotSizes(filename,sizes):\n",
    "\tsizes=sorted(sizes)\n",
    "\tipd = 1/plt.rcParams['figure.dpi'] \n",
    "\tplt.figure(figsize=(1024*ipd,768*ipd))\n",
    "\tplt.title(f\"{filename} #({len(sizes)}) m({sizes[0]}) M({sizes[-1]})\")\n",
    "\tplt.plot(range(len(sizes)), sorted(sizes))\n",
    "\tos.makedirs(os.path.dirname(filename),exist_ok=True)\n",
    "\tplt.savefig(filename)\n",
    "\tplt.show()\n",
    "\n",
    "if True:\n",
    "\tfor catalog, bucket in buckets:\n",
    "\t\tsizes=[it[0] for it in client.execute(f\"SELECT size FROM nsdf.catalog WHERE catalog='{catalog}' and bucket='{bucket}'\")]\n",
    "\t\tif not sizes: continue\n",
    "\t\tPlotSizes(filename=f\"/tmp/plots/{catalog}/{bucket}.png\",sizes=sizes)\n",
    "\t\t# remove the `break` if you want all the plots\n",
    "\t\tbreak "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total records 56M files 44PB\n",
    "client = Connect()\n",
    "TOT_FILES,TOT_BYTES=client.execute(f\"SELECT count(size),SUM(size)/(1024*1024*1024) FROM nsdf.catalog;\")[0]\n",
    "print(TOT_FILES,TOT_BYTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # total size per catalog/bucket\n",
    "client.execute(\"\"\"\n",
    "\tSELECT catalog,bucket, SUM(size) as TotSize\n",
    "\tFROM nsdf.catalog\n",
    "\tGROUP BY catalog,bucket\n",
    "\tORDER BY TotSize DESC;\n",
    "\t\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of objeccts per catalog/bucket\n",
    "client.execute(\"\"\"\n",
    "\tSELECT catalog,bucket, COUNT(size) As NumObjects\n",
    "\tFROM nsdf.catalog\n",
    "\tgroup by catalog,bucket\n",
    "\tORDER BY NumObjects DESC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of looking to a specific catalog,bucket\n",
    "client.execute(\"\"\"\n",
    "\tSELECT SUM(size) from nsdf.catalog \n",
    "\tWHERE catalog='aws-open-data' and bucket='noaa-cors-pds';\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIKE querry for looking into filenames\n",
    "client.execute(\"\"\"\n",
    "\tSELECT count(*) from nsdf.catalog\n",
    "\twhere name like '%a%'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size per catalog\n",
    "client.execute(\"\"\"\n",
    "\tSELECT catalog,SUM(size)\n",
    "\tFROM nsdf.catalog\n",
    "\tGROUP BY catalog;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUM per bucket\n",
    "client.execute(\"\"\"\n",
    "\tSELECT catalog,bucket,SUM(size)\n",
    "\tFROM nsdf.catalog\n",
    "\tGROUP BY catalog,bucket\n",
    "\tORDER BY COUNT(size) DESC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT per bucket \n",
    "client.execute(\"\"\"\n",
    "\tSELECT catalog,bucket,COUNT(size)\n",
    "\tFROM nsdf.catalog\n",
    "\tGROUP BY catalog,bucket\n",
    "\tORDER BY COUNT(size) DESC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file size distribution \n",
    "client.execute(\"\"\"\n",
    "\tSELECT size\n",
    "\tFROM nsdf.catalog\n",
    "\tWHERE catalog='mc' and bucket='102'\n",
    "\tORDER BY size ASC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete records\n",
    "# ALTER TABLE nsdf.catalog DELETE WHERE 1=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get extension\n",
    "client.execute(\"\"\"\n",
    "\tSELECT splitByChar('.','giorgio.scorzelli.h5')[-1]\n",
    "\tFROM nsdf.catalog             \n",
    "\tORDER BY size DESC\n",
    "\tLIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "from bokeh.layouts import row,column\n",
    "from bokeh.models.widgets import Div\n",
    "from bokeh.models import ColumnDataSource, Slider , Dropdown, Select, DataTable,TableColumn, Button, Dropdown\n",
    "from bokeh.plotting import figure, curdoc\n",
    "from bokeh.themes import Theme\n",
    "from bokeh.io import show, output_notebook, curdoc\n",
    "from bokeh.sampledata.sea_surface_temperature import sea_surface_temperature\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "def in_notebook():\n",
    "\tfrom IPython import get_ipython\n",
    "\treturn True if get_ipython() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_notebook():\n",
    "\toutput_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_table2=[]\n",
    "for name,count,size in repository_table:\n",
    "\trepository_table2.append([\n",
    "\t\tname,\n",
    "\t\tformat(count, \",\").rjust(11,'_') + \" (\"+AddSuffix(count, type=\"\", base=1000).ljust(7)+\")\",\n",
    "\t\tformat(size , \",\").rjust(23,'_') + \" (\"+AddSuffix (size ,type=\"B\",base=1024).ljust(7)+\")\"])\n",
    "\n",
    "query_result = GetFileNames(catalog = None,group_buckets = True, group_suffix = False)  \n",
    "\n",
    "CurrentValues     = {\n",
    "\t\"filter\": \"File_type\",\n",
    "\t\"repository\": \"all repositories\"\n",
    "}\n",
    "\n",
    "# /////////////////////////////////////////////////////////////////////\n",
    "def modify_doc(doc):\n",
    "\n",
    "\tdf = sea_surface_temperature.copy()\n",
    "\tsource = ColumnDataSource(data = df)\n",
    "\n",
    "\tsource_table = ColumnDataSource(data = {\n",
    "\t\t\"Repository\"   : [s[0] for s in repository_table2],\n",
    "\t\t\"file count\"   : [s[1] for s in repository_table2],\n",
    "\t\t\"total storage\": [s[2] for s in repository_table2],\n",
    "\t\t})\n",
    "\n",
    "\tbutton = Button(label=\" \", button_type=\"success\", height = 30)\n",
    "\n",
    "\tdropdown = Dropdown(label=\"Grouping\", button_type=\"warning\", menu=[\n",
    "\t\t(\"None\", \"None\"), \n",
    "\t\t(\"Bucket\", \"Bucket\"), \n",
    "\t\t(\"File_type\", \"File_type\")\n",
    "\t])\n",
    "\n",
    "\tdf2 = sea_surface_temperature.copy()\n",
    "\tsource2 = ColumnDataSource(data = df2)\n",
    "\n",
    "\tsource_table2 = ColumnDataSource(data = {\n",
    "\t\t\"repository\": [s[0] for s in query_result[1]],\n",
    "\t\t\"project\"   : [s[1] for s in query_result[1]],\n",
    "\t\t\"file type\" : [s[2] for s in query_result[1]],\n",
    "\t\t\"file count\": [s[3] for s in query_result[1]],\n",
    "\t\t\"storage\"   : [s[4] for s in query_result[1]]\n",
    "\t})\n",
    "\n",
    "\tplot = figure(\n",
    "\t\tx_axis_type = 'datetime', \n",
    "\t\ty_range = (0, 25),\n",
    "\t\ty_axis_label = 'Temperature (Celsius)',\n",
    "\t\ttitle = \"Sea Surface Temperature at 43.18, -70.43\")\n",
    "\n",
    "\tplot.line('time', 'temperature', source = source)\n",
    "\n",
    "\ttable = DataTable(\n",
    "\t\tsource = source_table,\n",
    "\t\theight=220,\n",
    "\t\tcolumns = [\n",
    "\t\t\tTableColumn(field = \"Repository\", title = \"Repository\"),\n",
    "\t\t\tTableColumn(field = \"file count\", title = \"file count\"),\n",
    "\t\t\tTableColumn(field = \"total storage\", title = \"total storage\")\n",
    "\t\t])\n",
    "\n",
    "\ttable2 = DataTable(\n",
    "\t\tsource = source_table2,\n",
    "\t\tcolumns = [\n",
    "\t\t\tTableColumn(field = \"repository\" , title = \"repository\"),\n",
    "\t\t\tTableColumn(field = \"project\"\t, title = \"project\"   ),\n",
    "\t\t\tTableColumn(field = \"file type\"  , title = \"file type\"   ),\n",
    "\t\t\tTableColumn(field = \"file count\" , title = \"file count\"   ),\n",
    "\t\t\tTableColumn(field = \"storage\"\t, title = \"storage\"   )\n",
    "\t\t])\n",
    "\n",
    "\tdef update_table2 ():\n",
    "\t\tpass\n",
    "\n",
    "\tdef UpdateTable():\n",
    "\n",
    "\t\tcatalog=None if CurrentValues[\"repository\"]== \"all repositories\" else CurrentValues[\"repository\"]\n",
    "\n",
    "\t\tquery_result = GetFileNames(catalog = catalog, group_buckets = False, group_suffix  = False)   \n",
    "\t\t\t\n",
    "\t\tsource_table2.data = {\n",
    "\t\t\t\"repository\": [s[0] for s in query_result[1]],\n",
    "\t\t\t\"project\"   : [s[1] for s in query_result[1]],\n",
    "\t\t\t\"file type\" : [s[2] for s in query_result[1]],\n",
    "\t\t\t\"file count\": [s[3] for s in query_result[1]],\n",
    "\t\t\t\"storage\"   : [s[4] for s in query_result[1]]}\n",
    "\n",
    "\t\n",
    "\tdef callback(attr, old, new, button = button):  \n",
    "\t\tprint(dropdown.menu,dropdown.select,dropdown.label,dropdown.name)\n",
    "\t\t#rint(dropdown.menu,dropdown.item)\n",
    "\t\tbutton.label = repository_table[new[0]][0]\n",
    "\t\tCurrentValues[\"repository\"] = repository_table[new[0]][0]\n",
    "\n",
    "\t\tcatalog=None if repository_table[new[0]][0]== \"all repositories\" else repository_table[new[0]][0]\n",
    "\n",
    "\t\tquery_result = GetFileNames(catalog = catalog, group_buckets = False,  group_suffix = False)   \n",
    "\n",
    "\t\tsource_table2.data = {\n",
    "\t\t\t\"repository\": [s[0] for s in query_result[1]],\n",
    "\t\t\t\"project\"   : [s[1] for s in query_result[1]],\n",
    "\t\t\t\"file type\" : [s[2] for s in query_result[1]],\n",
    "\t\t\t\"file count\": [s[3] for s in query_result[1]],\n",
    "\t\t\t\"storage\"   : [s[4] for s in query_result[1]]}\n",
    "\t\t\n",
    "\tsource_table.selected.on_change('indices', callback)\n",
    "\n",
    "#\t dropdown.on_click('value',callbackdd)\n",
    "\tdef handler(event):\n",
    "\t\tprint (CurrentValues[\"filter\"])\n",
    "\t\tprint(event.item)\n",
    "\t\tCurrentValues[\"filter\"]\t = event.item\n",
    "\t\tprint (CurrentValues[\"filter\"])\n",
    "\t\tUpdateTable()\n",
    "\n",
    "\tdropdown.on_click(handler)\t\n",
    "\n",
    "\tdoc.add_root(column(table,button, dropdown,table2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(modify_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# If needed uncomment the next line to Set environment variables based on warning from previous cell\n",
    "#os.environ['BOKEH_ALLOW_WS_ORIGIN'] = 'localhost:8889'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
