{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py, itertools\n",
    "import imageio as io\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import transform as sp\n",
    "\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = models.load_model('resources/nsdf/workflow/seg_msd_50_2_ep100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_image_into_tiles(img, main_size=380, border_offset=10, step=None):\n",
    "    true_step = main_size #+ border_offset #(Uncomment the second section of line when trying to average out the border offset)\n",
    "    img = np.pad(img,((border_offset,border_offset),(border_offset,border_offset)),'edge')    \n",
    "    x, y = img.shape[:2]\n",
    "    x_left = np.arange(border_offset, img.shape[0] - main_size + border_offset + 1, true_step)\n",
    "    x_left = x_left[0:-1] # Easy fix when 200 x 200 image with 100 pixel border offset is used (last element is not required)\n",
    "    y_top = np.arange(border_offset, img.shape[1] - main_size + border_offset + 1, true_step)\n",
    "    y_top = y_top[0:-1]\n",
    "    image_list = []\n",
    "    for x, y in itertools.product(x_left, y_top):\n",
    "        tile_i = img[x-border_offset:x+main_size+border_offset, y-border_offset:y+main_size+border_offset]\n",
    "        image_list.append(tile_i)\n",
    "    return np.array(image_list)  # without modifying x_left and y_top from list to ndarray gives error because of matrix mismatch\n",
    "\n",
    "\n",
    "def reassemble_tiled_image_noavg(img_stack, shape, main_size, border_offset):\n",
    "    result = np.zeros([shape[0],shape[1]])\n",
    "    true_step = main_size  \n",
    "    x_left = np.arange(0, result.shape[0] - main_size + border_offset + 1, true_step)\n",
    "    y_top = np.arange(0, result.shape[1] - main_size + border_offset + 1, true_step)\n",
    "    count = 0\n",
    "    # We just add the \n",
    "    for x, y in itertools.product(x_left, y_top):\n",
    "        result[x:x+main_size,y:y+main_size] = img_stack[count,border_offset:border_offset+main_size,border_offset:border_offset+main_size]\n",
    "        count = count + 1    \n",
    "    return result\n",
    "\n",
    "def normalize_crop_reshape_image(fpath):\n",
    "    crop = slice(250, 2250), slice(250, 2250)\n",
    "    img = io.imread(fpath)[crop].astype(float)\n",
    "    vmin, vmax = np.percentile(img, (0.01, 99.9))\n",
    "    img = np.clip(img - vmin, 0, vmax-vmin) / (vmax - vmin)\n",
    "    return img\n",
    "\n",
    "def process_image(src, dst, fname, main_size, border_offset):\n",
    "    \"\"\"\n",
    "    Read an image from os.path.join(src, fname), apply\n",
    "    CNN, and save to os.path.join(dst, fname)\n",
    "    \"\"\"\n",
    "    img = normalize_crop_reshape_image(os.path.join(src, fname))\n",
    "    img_tiles = cut_image_into_tiles(img, main_size, border_offset) # Mainsize + 2*Border Size should be 400 (Trained CNN model only takes 400 x 400 images)\n",
    "    img_segment_tile = []\n",
    "    for y in range(0,img_tiles.shape[0]):\n",
    "        img_seg = img_tiles[y,:] \n",
    "        img_seg = np.expand_dims(img_seg, (0, 3))\n",
    "        img_segment = trained_model.predict(img_seg)\n",
    "        img_segment_tile.append(np.squeeze(img_segment))    \n",
    "    img_segment_tile = np.array(img_segment_tile)\n",
    "    #img_segment_stitch = reassemble_tiled_image(img_segment_tile, [img.shape[0],img.shape[1]], main_size, border_offset)\n",
    "    img_segment_stitch = reassemble_tiled_image_noavg(img_segment_tile, [img.shape[0],img.shape[1]], main_size, border_offset)\n",
    "    img_norm = (img_segment_stitch*255).astype('uint8')\n",
    "    outpath = os.path.join(dst, fname)\n",
    "    try:\n",
    "        io.imsave(outpath,img_norm) #sp.resize(np.squeeze(img_norm),[2000,2000]))\n",
    "    except FileNotFoundError:\n",
    "        os.makedirs(dst)\n",
    "        io.imsave(outpath,img_norm) #sp.resize(np.squeeze(img_norm),[2000,2000]))\n",
    "    return img_norm\n",
    "\n",
    "def process_all_images_in_folder(src, dst, main_size, border_offset):\n",
    "    file_list = os.listdir(src)\n",
    "    for file in file_list:\n",
    "        print(file)\n",
    "        process_image(src, dst, file, main_size, border_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample ID: fly_scan_id_112517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the CNN Model for on one single Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the reconstructed slices are saved:\n",
    "src = '/home/kancr/ondemand/CNN_Model_Test/Reconstructed_Data/'\n",
    "# Where to save the processed images:\n",
    "dst = '/home/kancr/ondemand/CNN_Model_Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fname = 'recon_cgls_tv_01000.tiff'\n",
    "input_img = normalize_crop_reshape_image(os.path.join(src, fname ))\n",
    "img_segment = process_image(src, dst, fname, 200, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f, ((ax1, ax2)) = plt.subplots(2, 1, figsize=([20, 20]))\n",
    "\n",
    "mappable = ax1.imshow(np.squeeze(input_img), cmap = 'gray')\n",
    "f.colorbar(mappable, ax=ax1)\n",
    "\n",
    "mappable = ax2.imshow(np.squeeze(img_segment), cmap = 'gray')\n",
    "f.colorbar(mappable, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Small Sections in a given slice to check the performance of the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_left = np.arange(0, 2000 - 200 + 100 + 1, 200)\n",
    "print(x_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the reconstructed slices are saved:\n",
    "src = '/home/kancr/ondemand/CNN_Model_Test/Reconstructed_Data/'\n",
    "# Where to save the processed images:\n",
    "dst = '/home/kancr/ondemand/CNN_Model_Test/'#'TiledImages_SegmentedData/'\n",
    "fname = 'recon_cgls_tv_00801.tiff'\n",
    "input_img = normalize_crop_reshape_image(os.path.join(src, fname))\n",
    "img_section = input_img[1000:1400,1200:1600]\n",
    "img_section = np.expand_dims(img_section, (0, 3))\n",
    "img_segment_section = trained_model.predict(img_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f, ((ax1)) = plt.subplots(1, 1, figsize=([20, 20]))\n",
    "\n",
    "mappable = ax1.imshow(np.squeeze(input_img), cmap = 'gray')\n",
    "f.colorbar(mappable, ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f, ((ax1, ax2)) = plt.subplots(2, 1, figsize=([20, 20]))\n",
    "\n",
    "mappable = ax1.imshow(np.squeeze(img_section), cmap = 'gray')\n",
    "f.colorbar(mappable, ax=ax1)\n",
    "\n",
    "mappable = ax2.imshow(np.squeeze(img_segment_section), cmap = 'gray')\n",
    "f.colorbar(mappable, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the CNN Model Model for all the Reconstruction Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the reconstructed slices are saved:\n",
    "src = '/home/kancr/ondemand/CNN_Model_Test/Reconstructed_Data/'\n",
    "# Where to save the processed images:\n",
    "dst = '/home/kancr/ondemand/CNN_Model_Test/TiledImages_SegmentedData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double Check you are passing to appropriate destination\n",
    "# process_all_images_in_folder(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample ID: fly_scan_id_112509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the reconstructed slices are saved:\n",
    "src = '/home/kancr/ondemand/CNN_Model_Test/fly_scan_id_112509/'\n",
    "# Where to save the processed images:\n",
    "dst = '/home/kancr/ondemand/CNN_Model_Test/fly_scan_id_112509/Segment_Data'\n",
    "fname = 'recon_cgls_tv_01202.tiff'\n",
    "input_img = normalize_crop_reshape_image(os.path.join(src, fname))\n",
    "img_segment = process_image(src, dst, fname)\n",
    "\n",
    "f, ((ax1, ax2)) = plt.subplots(2, 1, figsize=([20, 20]))\n",
    "mappable = ax1.imshow(np.squeeze(input_img), cmap = 'gray')\n",
    "f.colorbar(mappable, ax=ax1)\n",
    "\n",
    "mappable = ax2.imshow(np.squeeze(img_segment), cmap = 'gray')\n",
    "f.colorbar(mappable, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the performance for small section in this sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = normalize_crop_reshape_image(os.path.join(src, fname))\n",
    "img_section = input_img[600:1000,900:1300]\n",
    "img_section = np.expand_dims(img_section, (0, 3))\n",
    "img_segment_section = trained_model.predict(img_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ((ax1, ax2)) = plt.subplots(2, 1, figsize=([20, 20]))\n",
    "mappable = ax1.imshow(np.squeeze(img_section), cmap = 'gray')\n",
    "f.colorbar(mappable, ax=ax1)\n",
    "mappable = ax2.imshow(np.squeeze(img_segment_section), cmap = 'gray')\n",
    "f.colorbar(mappable, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f, ((ax1, ax2)) = plt.subplots(1, 2, figsize=([20, 20]))\n",
    "\n",
    "mappable = ax1.imshow(np.squeeze(input_img))\n",
    "f.colorbar(mappable, ax=ax1)\n",
    "\n",
    "mappable = ax2.imshow(img_segment)\n",
    "f.colorbar(mappable, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 20))\n",
    "mappable = ax1.imshow(np.squeeze(input_img))\n",
    "f.colorbar(mappable, ax=ax1)\n",
    "# tmp = 255*np.squeeze(img_segment)\n",
    "# tmp01 = tmp<=200\n",
    "# tmp02 = tmp>25\n",
    "# tmp03 = np.bitwise_xor(tmp01,tmp02)\n",
    "mappable = ax2.imshow(dst1)\n",
    "f.colorbar(mappable, ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Averaging Filter\n",
    "import cv2\n",
    "av_filter = 1/400 * np.ones([20,20], dtype='float32')\n",
    "\n",
    "# # ddepth = -1, means destination image has depth same as input image\n",
    "dst1 = cv2.filter2D(img_segment, -1, av_filter)\n",
    "# cv2.imwrite('2_av_fil.jpg', dst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Row = np.arange(0, tmp.shape[0] - 390 + 1, 390)\n",
    "Col = np.arange(0, tmp.shape[0] - 390 + 1, 390)\n",
    "print(Row.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design a Mask to Correct the Overlap Region\n",
    "Mask = np.ones([2000,2000])\n",
    "Row_Ind = np.arange(0, Mask.shape[0] - 380 + 1, 390)\n",
    "print(Row_Ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 20))\n",
    "mappable = ax1.imshow(tmp)\n",
    "f.colorbar(mappable, ax=ax1)\n",
    "x_left = np.arange(0, tmp.shape[0] - 390 + 1, 390)\n",
    "tmp[x_left[4]:x_left[4]+9,:] = 255\n",
    "mappable = ax2.imshow(Mask)\n",
    "f.colorbar(mappable, ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
